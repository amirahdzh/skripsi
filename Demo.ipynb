{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_cJo_xkkTck"
      },
      "source": [
        "# DEMO PROGRAM SKRIPSI\n",
        "AMIRAH DZATUL HIMMAH <BR>\n",
        "2002871"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hAzcnJwH-5NX",
        "outputId": "09efe780-3474-47da-f20d-c30233f0832b"
      },
      "outputs": [],
      "source": [
        "!pip install keras-cv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NDKj5xHB-g_7",
        "outputId": "66230414-e54e-48be-d76f-9e1803b09f3c"
      },
      "outputs": [],
      "source": [
        "# insert dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!unzip '/content/drive/MyDrive/skripsi/dataset_split.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9pNRdZrEACl5",
        "outputId": "fb2f0742-212e-4965-b56f-e25cb8a513f8"
      },
      "outputs": [],
      "source": [
        "# Dictitonary for experiment configurations\n",
        "EXPERIMENT_CONFIGS = {\n",
        "    1: {\"name\": \"experiment_1_VanillaCnnN\", \"model\": \"VanillaCNN\", \"augment\": \"No\", \"trainable\": False},\n",
        "    2: {\"name\": \"experiment_2_VanillaCnnS\", \"model\": \"VanillaCNN\", \"augment\": \"Simple\", \"trainable\": False},\n",
        "    3: {\"name\": \"experiment_3_VanillaCnnR\", \"model\": \"VanillaCNN\", \"augment\": \"Randaugment\", \"trainable\": False},\n",
        "    4: {\"name\": \"experiment_4_InceptionV3NF\", \"model\": \"InceptionV3\", \"augment\": \"No\", \"trainable\": False},\n",
        "    5: {\"name\": \"experiment_5_InceptionV3NU\", \"model\": \"InceptionV3\", \"augment\": \"No\", \"trainable\": True},\n",
        "    6: {\"name\": \"experiment_6_InceptionV3SF\", \"model\": \"InceptionV3\", \"augment\": \"Simple\", \"trainable\": False},\n",
        "    7: {\"name\": \"experiment_7_InceptionV3SU\", \"model\": \"InceptionV3\", \"augment\": \"Simple\", \"trainable\": True},\n",
        "    8: {\"name\": \"experiment_8_InceptionV3RF\", \"model\": \"InceptionV3\", \"augment\": \"Randaugment\", \"trainable\": False},\n",
        "    9: {\"name\": \"experiment_9_InceptionV3RU\", \"model\": \"InceptionV3\", \"augment\": \"Randaugment\", \"trainable\": True},\n",
        "    10: {\"name\": \"experiment_10_ResNet50NF\", \"model\": \"ResNet50\", \"augment\": \"No\", \"trainable\": False},\n",
        "    11: {\"name\": \"experiment_11_ResNet50NU\", \"model\": \"ResNet50\", \"augment\": \"No\", \"trainable\": True},\n",
        "    12: {\"name\": \"experiment_12_ResNet50SF\", \"model\": \"ResNet50\", \"augment\": \"Simple\", \"trainable\": False},\n",
        "    13: {\"name\": \"experiment_13_ResNet50SU\", \"model\": \"ResNet50\", \"augment\": \"Simple\", \"trainable\": True},\n",
        "    14: {\"name\": \"experiment_14_ResNet50RF\", \"model\": \"ResNet50\", \"augment\": \"Randaugment\", \"trainable\": False},\n",
        "    15: {\"name\": \"experiment_15_ResNet50RU\", \"model\": \"ResNet50\", \"augment\": \"Randaugment\", \"trainable\": True},\n",
        "}\n",
        "\n",
        "# Select experiment\n",
        "selected_experiment = 14  # Change this number to select experiment\n",
        "\n",
        "# Set experiment configuration\n",
        "config = EXPERIMENT_CONFIGS[selected_experiment]\n",
        "EXPERIMENT_NAME = config[\"name\"]\n",
        "MODEL_NAME = config[\"model\"]\n",
        "AUGMENTATION_TYPE = config[\"augment\"]\n",
        "BASE_MODEL_TRAINABLE = config[\"trainable\"]\n",
        "\n",
        "print(f\"Running {EXPERIMENT_NAME} with:\")\n",
        "print(f\" - Model: {MODEL_NAME}\")\n",
        "print(f\" - Augmentation: {AUGMENTATION_TYPE}\")\n",
        "print(f\" - Base Model Trainable: {BASE_MODEL_TRAINABLE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ryw9X4dh6O4c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import layers\n",
        "from keras import callbacks\n",
        "from keras_cv.layers import RandAugment\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gC_EYDPzATZy"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "DRIVE_DIR = '/content/drive/MyDrive/skripsi/eksperimen' # Change this to your own directory\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 100\n",
        "\n",
        "# Adjust the image size based on the model\n",
        "def get_image_size(model_name):\n",
        "    if model_name == \"VanillaCNN\":\n",
        "        return 256  # Vanilla CNN needs 256x256\n",
        "    elif model_name == \"InceptionV3\":\n",
        "        return 299  # InceptionV3 needs 299x299\n",
        "    elif model_name == \"ResNet50\":\n",
        "        return 224  # ResNet50 needs 224x224\n",
        "    else:\n",
        "        raise ValueError(f\"Model {model_name} unrecognized\")\n",
        "\n",
        "IMAGE_SIZE = get_image_size(MODEL_NAME)\n",
        "\n",
        "TRAIN_DIR = 'dataset_split/train'\n",
        "VAL_DIR = 'dataset_split/val'\n",
        "TEST_DIR = 'dataset_split/test'\n",
        "\n",
        "# Paths\n",
        "EXPERIMENT_DIR = os.path.join(DRIVE_DIR, EXPERIMENT_NAME)\n",
        "os.makedirs(EXPERIMENT_DIR, exist_ok=True)\n",
        "HISTORY_PATH = os.path.join(EXPERIMENT_DIR, 'training_history.csv')\n",
        "BEST_MODEL_PATH = os.path.join(EXPERIMENT_DIR, 'best_model.keras')\n",
        "EPOCH_MODEL_PATH = os.path.join(EXPERIMENT_DIR, 'checkpoint.keras')\n",
        "PLOT_PATH = os.path.join(EXPERIMENT_DIR, 'plot.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hVOHNz-AV7t"
      },
      "outputs": [],
      "source": [
        "# Dataset Loader\n",
        "def load_dataset(data_dir, image_size):\n",
        "    data_dir = pathlib.Path(data_dir)\n",
        "    class_names = sorted([item.name for item in data_dir.glob('*') if item.is_dir()])\n",
        "    class_dict = {name: idx for idx, name in enumerate(class_names)}\n",
        "    images, labels = [], []\n",
        "\n",
        "    for class_name in class_names:\n",
        "        for image_path in (data_dir / class_name).glob('*'):\n",
        "            img = tf.keras.utils.load_img(image_path, target_size=image_size)\n",
        "            images.append(tf.keras.utils.img_to_array(img))\n",
        "            labels.append(class_dict[class_name])\n",
        "\n",
        "    return np.array(images), np.array(labels), class_names\n",
        "\n",
        "# Dataset Preparation\n",
        "def prepare_dataset(x, y, augment_type):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "    dataset = dataset.shuffle(len(x)).batch(BATCH_SIZE)\n",
        "\n",
        "    if augment_type == \"Simple\":\n",
        "        simple_aug = keras.Sequential(\n",
        "            [\n",
        "                layers.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
        "                layers.RandomFlip(\"horizontal\"),\n",
        "                layers.RandomRotation(factor=0.1),\n",
        "                layers.RandomBrightness(factor=0.2),\n",
        "            ]\n",
        "        )\n",
        "        dataset = dataset.map(lambda x, y: (simple_aug(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    elif augment_type == \"Randaugment\":\n",
        "        rand_augment = RandAugment(value_range=(0, 255), augmentations_per_image=3, magnitude=0.5)\n",
        "        dataset = dataset.map(lambda x, y: (rand_augment(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Model Vanilla CNN\n",
        "def build_vanilla_cnn_model(num_classes):\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3)),\n",
        "\n",
        "        # 1st Conv Layer + Max Pooling\n",
        "        layers.Conv2D(32, (3, 3), activation='relu'),  # 32 filters, kernel size 3x3\n",
        "        layers.MaxPooling2D(2, 2),  # Max Pooling with 2x2 pool size\n",
        "\n",
        "        # 2nd Conv Layer + Max Pooling\n",
        "        layers.Conv2D(32, (3, 3), activation='relu'),  # 32 filters, kernel size 3x3\n",
        "        layers.MaxPooling2D(2, 2),  # Max Pooling with 2x2 pool size\n",
        "\n",
        "        # 3rd Conv Layer + Max Pooling\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),  # 64 filters, kernel size 3x3\n",
        "        layers.MaxPooling2D(2, 2),  # Max Pooling with 2x2 pool size\n",
        "\n",
        "        # 4th Conv Layer + Max Pooling\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),  # 64 filters, kernel size 3x3\n",
        "        layers.MaxPooling2D(2, 2),  # Max Pooling with 2x2 pool size\n",
        "\n",
        "        # Flatten the output of the last Conv layer to feed into the Dense layers\n",
        "        layers.Flatten(),\n",
        "\n",
        "        # First Dense Layer\n",
        "        layers.Dense(128, activation='relu'),  # Fully connected layer with 128 units\n",
        "\n",
        "        # Second Dense Layer (Output Layer for 5 categories)\n",
        "        layers.Dense(num_classes, activation='softmax')  # Output layer with 5 units for 5 categories\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Model Definition\n",
        "def build_model(model_name, num_classes, trainable):\n",
        "    image_size = get_image_size(model_name)  # Get image size based on model\n",
        "\n",
        "    if model_name == \"VanillaCNN\":\n",
        "        # if model_name is VanillaCNN, build the model using the function\n",
        "        return build_vanilla_cnn_model(num_classes)\n",
        "\n",
        "    elif model_name == \"ResNet50\":\n",
        "        base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))\n",
        "\n",
        "    elif model_name == \"InceptionV3\":\n",
        "        base_model = tf.keras.applications.InceptionV3(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))\n",
        "\n",
        "    else:\n",
        "        # if model_name is not recognized, raise an error\n",
        "        raise ValueError(f\"Model {model_name} not supported in this configuration.\")\n",
        "\n",
        "    # Set base model to trainable or not\n",
        "    base_model.trainable = trainable\n",
        "\n",
        "    # Input layer\n",
        "    inputs = layers.Input(shape=(image_size, image_size, 3))\n",
        "\n",
        "    # Preprocessing based on model\n",
        "    if model_name == \"ResNet50\":\n",
        "        x = tf.keras.applications.resnet50.preprocess_input(inputs)\n",
        "    elif model_name == \"InceptionV3\":\n",
        "        x = tf.keras.applications.inception_v3.preprocess_input(inputs)\n",
        "\n",
        "    # Process the input through the base model\n",
        "    x = base_model(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x)  # Global Average Pooling\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)  # Classification layer\n",
        "\n",
        "    # Kembalikan model\n",
        "    return tf.keras.Model(inputs, outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rqw-SYwTAYqW",
        "outputId": "50a05338-5963-4dc3-fc2c-d406ba88b500"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset size: 335\n",
            "Validation dataset size: 82\n",
            "Test dataset size: 102\n"
          ]
        }
      ],
      "source": [
        "# Load datasets\n",
        "(x_train, y_train, _), (x_val, y_val, _), (x_test, y_test, _) = (\n",
        "    load_dataset(TRAIN_DIR, (IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    load_dataset(VAL_DIR, (IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    load_dataset(TEST_DIR, (IMAGE_SIZE, IMAGE_SIZE))\n",
        ")\n",
        "\n",
        "train_ds = prepare_dataset(x_train, y_train, AUGMENTATION_TYPE)\n",
        "val_ds = prepare_dataset(x_val, y_val, \"No\")\n",
        "test_ds = prepare_dataset(x_test, y_test, \"No\")\n",
        "\n",
        "print(f\"Train dataset size: {len(x_train)}\")\n",
        "print(f\"Validation dataset size: {len(x_val)}\")\n",
        "print(f\"Test dataset size: {len(x_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "collapsed": true,
        "id": "7rOyLLzjAine",
        "outputId": "55c4790e-38a7-476f-ee9b-4997441169f7"
      },
      "outputs": [],
      "source": [
        "# Resume Training or Initialize Model\n",
        "initial_epoch = 0\n",
        "latest_epoch_model = None\n",
        "\n",
        "# Check for the latest epoch checkpoint\n",
        "if os.path.exists(EXPERIMENT_DIR):\n",
        "    if os.path.exists(EPOCH_MODEL_PATH):\n",
        "        history_df = pd.read_csv(HISTORY_PATH)\n",
        "        last_epoch = history_df['epoch'].max()\n",
        "        print(f\"Epoch terakhir: {last_epoch}\")\n",
        "        initial_epoch = last_epoch + 1\n",
        "        print(f\"Resuming training from epoch {initial_epoch}\")\n",
        "        model = tf.keras.models.load_model(EPOCH_MODEL_PATH)\n",
        "\n",
        "    else:\n",
        "        print(\"No checkpoint found. Building new model.\")\n",
        "        model = build_model(MODEL_NAME, len(np.unique(y_train)), BASE_MODEL_TRAINABLE)\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dXBxNKYp8Bb-",
        "outputId": "f00ccdf8-dd70-4eb4-8a98-74561da25f9b"
      },
      "outputs": [],
      "source": [
        "# Check if CSV log exists\n",
        "if os.path.exists(HISTORY_PATH):\n",
        "    import pandas as pd\n",
        "    try: # Try to read the CSV file, handling potential errors\n",
        "        log_data = pd.read_csv(HISTORY_PATH)\n",
        "        if not log_data.empty:\n",
        "            initial_epoch = log_data['epoch'].max() + 1\n",
        "            print(f\"Resuming training from epoch {initial_epoch}\")\n",
        "    except pd.errors.EmptyDataError: # If the file is empty, skip loading and proceed\n",
        "        print(\"CSV log file is empty. Starting training from epoch 0.\")\n",
        "\n",
        "# Callbacks\n",
        "callbacks_list = [\n",
        "    callbacks.ModelCheckpoint(\n",
        "        filepath=BEST_MODEL_PATH,\n",
        "        save_best_only=True,\n",
        "        save_weights_only=False,\n",
        "        monitor='val_accuracy',\n",
        "        mode='max',\n",
        "        verbose=1\n",
        "    ),\n",
        "    callbacks.ModelCheckpoint(\n",
        "        filepath=EPOCH_MODEL_PATH,\n",
        "        save_best_only=False,\n",
        "        save_weights_only=False,\n",
        "        verbose=1\n",
        "    ),\n",
        "    callbacks.CSVLogger(HISTORY_PATH, append=True),\n",
        "]\n",
        "\n",
        "# Training\n",
        "print(f\"Training for {EXPERIMENT_NAME}\")\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    initial_epoch=initial_epoch,\n",
        "    callbacks=callbacks_list\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "collapsed": true,
        "id": "q3U-nkBb0QHq",
        "outputId": "ce43d83e-7f3d-4381-f42f-fd8503d0403e"
      },
      "outputs": [],
      "source": [
        "def plot_history_from_csv(csv_path, plot_path):\n",
        "    # Load training history from CSV\n",
        "    history_df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Plot training and validation accuracy\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history_df['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history_df['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.legend()\n",
        "    plt.title(f\"Accuracy {EXPERIMENT_NAME}\")\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "\n",
        "    # Plot training and validation loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history_df['loss'], label='Train Loss')\n",
        "    plt.plot(history_df['val_loss'], label='Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.title(f\"Loss {EXPERIMENT_NAME}\")\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "\n",
        "    # Save the plot\n",
        "    plt.savefig(plot_path)\n",
        "    plt.show()\n",
        "\n",
        "# Call the function using CSV_LOG_PATH and PLOT_PATH\n",
        "plot_history_from_csv(HISTORY_PATH, PLOT_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AyiHekAH0SAQ",
        "outputId": "ff27bad9-39a9-4de2-ed41-fe776d9d3163"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_ds):\n",
        "    all_labels, all_preds = [], []\n",
        "    inference_times = []  # List to store inference times\n",
        "\n",
        "    for x, y in test_ds:\n",
        "        start_time = time.time()  # Record start time\n",
        "        preds = model.predict(x, verbose=1)\n",
        "        end_time = time.time()  # Record end time\n",
        "        inference_times.append(end_time - start_time)  # Calculate and store inference time\n",
        "\n",
        "        all_preds.extend(np.argmax(preds, axis=1))\n",
        "        all_labels.extend(y.numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='macro')\n",
        "    recall = recall_score(all_labels, all_preds, average='macro')\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "    avg_inference_time = np.mean(inference_times)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"Average Inference Time: {avg_inference_time:.4f} seconds\")\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "evaluate_model(model, test_ds)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
